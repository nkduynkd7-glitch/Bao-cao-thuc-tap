
# Cluster proxmox (Yêu cầu 3 node proxmox, đảm bảo khi 1 node chết VM vẫn chạy bình thường)  
## Triển khai Proxmox Cluster bằng 1 node vật lý và 2 node VM

### Thông tin tổng quan
Mục tiêu của bài thực hành:
Tạo 2 máy ảo Proxmox (node2 và node3) trên Proxmox node chính  
Cài Proxmox VE lên 2 máy ảo  
Tạo Proxmox Cluster từ node chính  
Add 2 node VM vào cluster để hình thành cluster nhiều node  

Mô hình triển khai:
Node 1 (máy thật) tên `nvtuprx`  
Node 2 (VM) tên `nodes2`  
Node 3 (VM) tên `nodes3`  

---

## 1 Tạo 2 máy ảo pve2 và pve3 trên node chính
### 1.1 Upload ISO Proxmox VE
Trên Web UI của node chính:
Datacenter → local (nvtuprx) → Content → Upload  
Chọn file ISO `proxmox-ve_9.1-1.iso` và upload lên storage `local`

<img width="1582" height="579" alt="image" src="https://github.com/user-attachments/assets/2e0ee16b-2596-4b3e-8ad8-d7ec2ba8cf83" />


Sau khi upload thành công, ISO sẽ xuất hiện trong danh sách Content.

### 1.2 Tạo VM pve2
Thao tác:
Bấm Create VM

Thông số cấu hình đề xuất:
General  
VM ID: 202  
Name: notes2  

OS  
Use CD DVD disc image file (iso)  
Storage: local  
ISO image: proxmox ve 9.1 1 iso  

System  
BIOS: SeaBIOS  
Machine: q35  
SCSI Controller: VirtIO SCSI single  

Disks  
Disk system chọn storage `sdc`  
Dung lượng gợi ý 16GB  
Bus Device: SCSI  

CPU  
Cores 2  
Type host  

Memory  
4096 MB  
Tắt Ballooning

Network  
Bridge: vmbr0  
Model: VirtIO

### 1.3 Tạo VM notes3
Tạo tương tự notes2, đổi tên thành `notes3` và VM ID khác (ví dụ 203)

---

## 2 Tải và cài Proxmox VE trên 2 máy ảo
### 2.1 Cài Proxmox VE trên notes2
Start VM notes2  
Mở Console và chạy Proxmox VE Installer

Các bước chính:
Chọn Install Proxmox VE  
Chọn đúng disk system (ví dụ /dev/sda 16GB)  
Không chọn nhầm disk dữ liệu nếu có

Đặt hostname và IP:
Hostname: pve2  
IP address: 172.16.5.62/24  
Gateway: 172.16.5.1  
DNS: DNS nội bộ hoặc 8.8.8.8

Sau khi cài xong, hệ thống hiển thị truy cập Web UI:
https://172.16.5.62:8006

<img width="915" height="285" alt="image" src="https://github.com/user-attachments/assets/e828f46d-cf9d-427b-a08e-c1a3351a4ea9" />


### 2.2 Cài Proxmox VE trên notes3
Start VM pve3 và làm tương tự notes2

Đặt hostname và IP:
Hostname: pve3  
IP address: 172.16.5.63/24  
Gateway: 172.16.5.1  
DNS: DNS nội bộ hoặc 8.8.8.8

Sau khi cài xong, truy cập Web UI:
https://172.16.5.63:8006

<img width="954" height="280" alt="image" src="https://github.com/user-attachments/assets/ef324266-c565-4838-9ec4-73227834a30d" />


---

## 3 Tạo Cluster trên node chính (nvtupvx)
### 3.1 Tạo cluster
Trên node chính (node1), chạy lệnh:
```bash
pvecm create labcluster
```

Kiểm tra cluster:
```bash
pvecm status
```

Kết quả mong đợi:
Cluster name: labcluster  
Nodes: 1  
Quorate: Yes  

---

## 4 Add notes2 và notes3 vào cluster
### 4.1 Add node notes2 vào cluster
Trên pve2 chạy:
```bash
pvecm add 172.16.5.50
```

Nhập mật khẩu `root` của node chính khi được hỏi.

Kiểm tra:
```bash
pvecm status
```

### 4.2 Add node notes3 vào cluster
Trên pve3 chạy:
```bash
pvecm add 172.16.5.50
```

Nhập mật khẩu `root` của node chính khi được hỏi.

---

### 5 Kiểm tra trạng thái cluster sau khi add node
Trên node chính (hoặc bất kỳ node nào) chạy:
```bash
pvecm status
pvecm nodes
```

Kết quả mong đợi:
Nodes: 3  
Quorate: Yes  
Danh sách node hiển thị đầy đủ:
nvtupvx  
notes2  
notes3  

<img width="466" height="500" alt="image" src="https://github.com/user-attachments/assets/8290ddf2-9260-47fe-a482-8eb08b366f7e" />


## Yêu cầu 3 node proxmox, đảm bảo khi 1 node chết VM vẫn chạy bình thường

### 1. Tạo ổ OSD: Tạo trên mỗi node
1. Dọn dẹp ổ để tạo ổ OSD: 

<img width="598" height="399" alt="image" src="https://github.com/user-attachments/assets/44619672-5f2e-4dcf-8ce9-152130d50d9d" />

```yaml
# Lệnh này chỉ nhắm vào ổ sdb, cực kỳ an toàn
dmsetup remove -f vg_sdb-data
dmsetup remove -f vg_sdb-data_tdata
dmsetup remove -f vg_sdb-data_tmeta
```

```yaml
vgremove -y vg_sdb
```

```yaml
# Xóa sạch mọi chữ ký (signatures) trên đĩa
wipefs -a -f /dev/sdb

# Ép Kernel nạp lại bảng phân vùng mới (trống)
partprobe /dev/sdb
```
kiểm tra lại
```yaml
lsblk
```

2. Sau khi dọn sạch ổ sdb, dùng cú pháp để tạo ổ OSD
```yaml
ceph-volume lvm create --data /dev/sdb1
```

### 2. Sau khi dọn ổ sẽ đi vào thực hiện cấu hình Ceph và HA
### Bước 1: Thiết lập Lưu trữ Tập trung (Ceph Storage)
Để HA hoạt động, dữ liệu máy ảo phải được truy cập từ mọi node.
1. **Khởi tạo OSD:** Mỗi node đóng góp 1 ổ đĩa vật lý (HDD/SSD) để làm OSD.

<img width="1608" height="705" alt="image" src="https://github.com/user-attachments/assets/b3de9cc0-4109-47ed-9491-5ee6c2faf0e9" />

<img width="1610" height="706" alt="image" src="https://github.com/user-attachments/assets/6248b3e3-7901-44df-abb9-f881a3feb7e9" />

3. **Tạo Pool:** Thiết lập pool `pool-ceph-VM` với các thông số an toàn:
   - **Size = 3**: Dữ liệu được nhân bản trên cả 3 node.
   - **Min Size = 2**: Đảm bảo an toàn, cho phép ghi khi ít nhất 2 node hoạt động.
   - **PG Autoscaler**: Bật (on) để tự động tối ưu hóa hiệu suất.
  
<img width="1609" height="695" alt="image" src="https://github.com/user-attachments/assets/3d97b5cb-7893-470c-9b7e-b2e4b1428741" />

<img width="1615" height="708" alt="image" src="https://github.com/user-attachments/assets/7ab967be-e417-4d42-af4a-defd64721ddc" />

3. **Di chuyển Disk**: Chuyển ổ cứng các VM (103, 301, 401) từ local sang `pool-ceph-VM`.

<img width="1610" height="194" alt="image" src="https://github.com/user-attachments/assets/6c1dfa2a-9dc8-40c9-a2be-d8bb7496c89e" />


### Bước 2: Cấu hình High Availability (HA)
Thiết lập cơ chế tự động phục hồi máy ảo khi node chết.
1. **HA Resources**: Thêm VM vào danh sách quản lý HA với trạng thái `started`.
2. **Quorum**: Đảm bảo ít nhất 2/3 node hoạt động để duy trì quyền biểu quyết (Quorum: OK).
3. **HA Group (Tùy chọn)**: Cấu hình nhóm ưu tiên nếu muốn máy ảo tự động quay về node gốc (Failback).

<img width="1611" height="697" alt="image" src="https://github.com/user-attachments/assets/4ea5ba85-9c0e-4e6a-9d2b-f566b664d426" />



## 3. Kết Quả Thử Nghiệm Thực Tế
* **Kịch bản**: Tắt đột ngột `node4`.
* **Diễn biến**:
   - Hệ thống phát hiện node chết qua giao thức Corosync.
   - HA Manager trên các node còn sống xác lập lại Quorum.
   - Máy ảo 401 tự động được khởi động lại trên `node3` hoặc `nvtuprx` mà không cần can thiệp thủ công.
  
<img width="302" height="315" alt="image" src="https://github.com/user-attachments/assets/094c2a6e-9ada-49e1-9ecb-e4bc534f458f" />



## 4. Hướng dẫn Bảo trì & Khắc phục lỗi
1. **Xử lý Health Warning**: Khi Monitor báo thiếu dung lượng (`low on available space`), thực hiện dọn dẹp log bằng lệnh `journalctl --vacuum-time=1d`.
2. **Xử lý Error State**: Nếu VM rơi vào trạng thái lỗi, cần `Disable` HA cho VM đó, sửa lỗi khởi động, rồi mới `Enable` lại.
3. **Mạng Cluster**: Luôn đảm bảo mạng truyền thông giữa các node ổn định để tránh mất Quorum dẫn đến treo máy ảo.






